{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name='./Wave', seed=1, side_channels=[channel])\n",
    "channel.set_configuration_parameters(time_scale = 20)\n",
    "print(\"WAVE environment created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 64\n",
    "l2 = 150\n",
    "l3 = 150\n",
    "l4 = 2\n",
    "\n",
    "model_policy = torch.nn.Sequential(\n",
    "  torch.nn.Linear(l1, l2),\n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(l2,l3),\n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(l3,l4),\n",
    "  torch.nn.Softmax(dim=1)\n",
    ")\n",
    "learning_rate = 1e-4\n",
    "optimizer_policy = torch.optim.Adam(model_policy.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 64\n",
    "l2 = 150\n",
    "l3 = 1\n",
    "\n",
    "model_value = torch.nn.Sequential(\n",
    "  torch.nn.Linear(l1, l2),\n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(l2, l3),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer_value = torch.optim.Adam(model_value.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards: np.ndarray, gamma):\n",
    "    reversed = np.copy(rewards)[::-1]\n",
    "    discounted_rewards = []\n",
    "    for i, reward in enumerate(reversed):\n",
    "        discounted_rewards.append(reward + (0 if i == 0 else reversed[i - 1]))\n",
    "        reversed[i] = reward * gamma\n",
    "        if i > 0:\n",
    "            reversed[i] += reversed[i - 1] * gamma\n",
    "    discounted_rewards = np.array(discounted_rewards[::-1])\n",
    "    # discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "    return discounted_rewards\n",
    "\n",
    "def loss_fn(predictions, advantages):\n",
    "    return -1 * torch.mean(advantages * torch.log(predictions))\n",
    "\n",
    "def preprocess_input(inp):\n",
    "    return np.append(inp.obs[1], inp.obs[0], axis=1).reshape(-1)\n",
    "\n",
    "def get_trajectories(model, max_iter=300):\n",
    "    states = []\n",
    "    actions = []\n",
    "    action_sets = []\n",
    "    rewards = []\n",
    "\n",
    "    env.reset()\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "    timestep = 0\n",
    "    while timestep < max_iter:\n",
    "        timestep += 1\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        if (len(terminal_steps) > 0):\n",
    "            break\n",
    "\n",
    "        state = preprocess_input(decision_steps)\n",
    "\n",
    "        states.append(state)\n",
    "        pred = model(torch.Tensor(np.array([state])))\n",
    "\n",
    "        action = np.random.choice(np.array([0, 1]), p=pred.detach().numpy().flatten())\n",
    "        actions.append(action)\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(np.array([[action]]))\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "        \n",
    "        new_decision_steps, new_terminal_steps = env.get_steps(behavior_name)\n",
    "        current_step = new_decision_steps if len(new_terminal_steps) == 0 else new_terminal_steps\n",
    "        reward = current_step.reward[0]\n",
    "        rewards.append(float(reward))\n",
    "\n",
    "    return torch.from_numpy(np.array(states)).float(), np.array(actions), np.array(rewards), np.array(action_sets), timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_advantages(values, masks, rewards):\n",
    "    adv = np.zeros(len(values))\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        next_value = 0\n",
    "        if i + 1 < len(rewards):\n",
    "            next_value = values[i + 1]\n",
    "        delta = rewards[i] + GAMMA * next_value * masks[i] - values[i]\n",
    "        adv[i] = delta\n",
    "\n",
    "    adv = np.array(adv)\n",
    "    return (adv - np.mean(adv)) / (np.std(adv) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 2000\n",
    "GAMMA = 0.9\n",
    "all_rewards = []\n",
    "all_timesteps = []\n",
    "all_actor_losses = []\n",
    "all_critic_losses = []\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    states, actions, rewards, action_sets, timestep = get_trajectories(model_policy)\n",
    "    current_reward = np.sum(rewards)\n",
    "    all_rewards.append(current_reward)\n",
    "    all_timesteps.append(timestep)\n",
    "    print(f'EPOCH: {i}, total reward: {current_reward}, timestep: {timestep}')\n",
    "\n",
    "    predictions = model_policy(states)\n",
    "    discounted_rewards = torch.tensor(discount_rewards(rewards, GAMMA))\n",
    "\n",
    "    values = model_value(states)\n",
    "    critic_loss = 0.5 * torch.pow(values - discounted_rewards, 2).mean()\n",
    "\n",
    "    detached_values = values.detach().numpy()\n",
    "    masks = np.ones_like(detached_values)\n",
    "    masks[-1] = 0\n",
    "    advantages = torch.Tensor(get_advantages(detached_values.flatten(), masks, rewards))\n",
    "    actions = torch.tensor(actions.reshape(-1, 1)).long()\n",
    "    prob_batch = predictions.gather(dim=1,index=actions).squeeze()\n",
    "    actor_loss = (advantages * -torch.log(prob_batch)).mean()\n",
    "\n",
    "    all_actor_losses.append(actor_loss.item())\n",
    "    all_critic_losses.append(critic_loss.item())\n",
    "\n",
    "    optimizer_value.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    optimizer_value.step()\n",
    "\n",
    "    optimizer_policy.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    optimizer_policy.step()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_per_x_element(data, x=10):\n",
    "    avg = []\n",
    "    sum = 0\n",
    "    for i, el in enumerate(data):\n",
    "        sum += el\n",
    "        if i % x == 0:\n",
    "            avg.append(sum / x)\n",
    "            sum = 0\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_per_x_element(all_critic_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_per_x_element(all_actor_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_per_x_element(all_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_per_x_element(all_timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name='./Wave', seed=1, side_channels=[channel])\n",
    "channel.set_configuration_parameters(time_scale = 3)\n",
    "print(\"WAVE environment created.\")\n",
    "\n",
    "i = 0\n",
    "env.reset()\n",
    "while True:\n",
    "    i += 1\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    if (len(terminal_steps) > 0):\n",
    "        break\n",
    "\n",
    "    preds = model_policy(torch.Tensor([preprocess_input(decision_steps)])).detach().numpy()\n",
    "    preds_value = model_value(torch.Tensor([preprocess_input(decision_steps)])).detach().numpy()\n",
    "    print(f'TIMESTEP {i}, policy {preds}, values {preds_value}')\n",
    "\n",
    "    action = np.argmax(preds)\n",
    "    action_tuple = ActionTuple()\n",
    "    action_tuple.add_discrete(np.array([[action]]))\n",
    "    env.set_actions(behavior_name, action_tuple)\n",
    "    env.step()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca9c2878fbaf906546635f1e51bfc073e4e43dd47adf91e04b336b35cf7c5ff3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
